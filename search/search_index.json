{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ibmas-watsonxdata","text":"<p>Please visit: https://pages.github.ibm.com/alexander/ibmas-watsonxdata for a better style.</p> <p>Welcome to the GitHub documentation for setting up a Red Hat Single Node Cluster!  In this guide, we will provide you with step-by-step instructions to deploy a powerful Red Hat OpenShift single-node cluster. This cluster serves as a fundamental building block for various applications, offering an environment that combines the benefits of containerization and orchestration.</p> <p>We'll guide you in the installation and deployment of an IBM Watsonx.data application. This powerful integration will showcase how your Red Hat OpenShift environment can host and support sophisticated applications, opening doors to new possibilities in data management, analysis, and insights.</p> <p>Whether you're a beginner seeking to explore the world of containerized applications or an experienced developer looking to harness the potential of Red Hat's technology stack, this documentation is tailored to meet your needs. Get ready to embark on a journey that blends Red Hat's robust infrastructure with the innovative capabilities of IBM WatsonX Data, creating a dynamic and versatile environment for your application development needs. Let's dive in and get started!</p>"},{"location":"#chapter-0-introduction","title":"Chapter 0 - Introduction","text":"<p>In the next chapters, we'll guide you through preparing your environment for the Red Hat Single Node Cluster (SNO) and installing an IBM watsonx.data standalone instance.</p> <p>To get started, follow these steps:</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"Chapter Content Step 1: Requirements Introduction to Requirements Step 2: Prepare the Installation Guide to Preparing the Installation using the bastion instance Step 3: Execute the Installation of watsonx.data Installing watsonx.data as standalone version <p>Continue to the Next Chapter.</p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/","title":"Step 3 - Install watsonx.data","text":""},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#navigation","title":"Navigation","text":"<p>Previous Chapter or return to the Introduction.</p> <p>in this comprehensive chapter, we will provide you with step-by-step instructions for successfully installing the IBM WatsonX Data application on your Red Hat Single Node Cluster (SNO). By the end of this chapter, you will have a clear understanding of how to set up and configure the WatsonX Data application on your cluster.</p> <p>Building upon the introduction and prerequisites covered earlier, this installation guide will delve into the specific details necessary for a seamless and successful deployment. The focus will primarily be on the installation of the WatsonX Data application, ensuring that you have all the information you need to carry out the process efficiently.</p> <p>By following the instructions outlined in this chapter, you will gain practical insights into provisioning the necessary resources, configuring the environment, and executing the installation process step by step. Our goal is to empower you with the knowledge and confidence to effectively set up the WatsonX Data application, enhancing your data-related capabilities within your Red Hat Single Node Cluster.</p> <p>Let's dive into the installation process and embark on the journey to unleash the full potential of IBM WatsonX Data within your cluster environment.</p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#31-prepare-the-installation","title":"3.1 Prepare the installation","text":"<p>The screen command is used to manage terminal sessions. To start a new session, use screen -S session_name, and to reattach to an existing session, use screen -r session_name. You can detach from a session by pressing Ctrl-a followed by d, and reattach using the reattach command. This enables you to run processes in the background, detach and reattach as needed. <pre><code>  screen -S &lt;name&gt; creates a new shell\n</code></pre> with  <pre><code>  screen -r &lt;name&gt; you can always attach to the shell if you lost the shell\nusing **screen -list** you can check for all open shells\n</code></pre> Example we use: <pre><code>screen -S installwatsonxdata\n</code></pre></p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#32-login-to-the-cluster","title":"3.2 login to the cluster","text":"<pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage login-to-ocp \\\n--user=${OCP_USERNAME} \\\n--password=${OCP_PASSWORD} \\\n--server=${OCP_URL}\n</code></pre>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#321-set-up-the-topology-run-the-following-commands","title":"3.2.1 Set up the topology. Run the following commands:","text":"<p><pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage apply-cluster-components\n</code></pre> <pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage authorize-instance-topology\n/root/ibm-lh-manage/ibm-lakehouse-manage setup-instance-topology\n</code></pre></p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#322-run-the-following-command-to-install-watsonxdata-cartridge-cr-and-accept-the-license-agreement","title":"3.2.2 Run the following command to install Watsonx.data cartridge (CR) and accept the license agreement:","text":"<pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage install --license_acceptance=true\n</code></pre>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#4-run-the-following-command-to-verify-whether-the-catalog-source-is-created","title":"4. Run the following command to verify whether the catalog source is created.","text":"<pre><code>oc get catalogsource -n ${PROJECT_CPD_OPS}\noc get csv -n ${PROJECT_CPD_OPS}\noc get po -n ibm-cert-manager\noc get po -n ibm-licensing\n</code></pre>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#41-verify-the-cr-status-run-the-following-command","title":"4.1 Verify the CR status. Run the following command:","text":"<p><pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage get-cr-status\n</code></pre> <pre><code># getting status for all installed components...  optionally add --components=&lt;comma separated list of cpd components&gt; for a specific set\n# component,CR-kind,CR-name,status,version,creationtimestamp,reconciled-version,operator-info\nwatsonx_data,WxdAddon,wxdaddon,Completed,1.0.1,2023-08-14T15:17:36Z,1.0.1,IBM watsonx.data operator 1.0.1 build number v1.0.1-1054-20230721-214944-onprem-v1.0.1\nanalyticsengine,AnalyticsEngine,analyticsengine-sample,Completed,4.7.1,2023-08-14T14:52:23Z,4.7.1,268\ncpd_platform,Ibmcpd,ibmcpd-cr,Completed,4.7.1,2023-08-14T14:07:05Z,--,cpdPlatform operator 4.1.0 build 10\nzen,ZenService,lite-cr,Completed,5.0.0,2023-08-14T14:10:05Z,5.0.0,zen operator 5.0.0 build 277\n</code></pre></p> <p><pre><code>oc get $(oc get Wxdaddon -o name -n ${PROJECT_CPD_INSTANCE}) -o custom-columns='VERSION:status.version,STATUS:status.wxdStatus,BUILD:.status.wxdBuildNumber' -n ${PROJECT_CPD_INSTANCE}\n</code></pre> Example <pre><code>VERSION   STATUS      BUILD\n1.0.1     Completed   IBM watsonx.data operator 1.0.1 build number v1.0.1-1054-20230721-214944-onprem-v1.0.1\n</code></pre></p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#42-get-the-login-credentials-to-access-ibm-cockpit-url-zen","title":"4.2 Get the Login credentials to access IBM Cockpit URL (ZEN)","text":"<p><pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage get-cpd-instance-details\n</code></pre> Example <pre><code>CPD Url: cpd-watsonxdata1-instance.apps.64da1ffc1bedbf00175f38c9.cloud.techzone.ibm.com\nCPD Username: admin\nCPD Password: 5SieL9rI6NFS\n</code></pre></p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#421-get-the-watsonxdata-external-url","title":"4.2.1 Get the watsonx.data external URL","text":"<pre><code>oc get routes -A | grep cpd | awk '{print \"https://\" $3}'\n</code></pre>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#5-validate-memory","title":"5. Validate Memory","text":"<p><pre><code>oc adm top node\n</code></pre> <pre><code>NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nmaster-1   2875m        9%     25062Mi         19%\n</code></pre> <pre><code>oc adm top pod -n $PROJECT_CPD_INSTANCE\n</code></pre> <pre><code>NAME                                               CPU(cores)   MEMORY(bytes)\nibm-lh-lakehouse-hive-metastore-76cf5d8fd5-68hns   10m          329Mi\nibm-lh-lakehouse-minio-67cc69976b-8pvvk            0m           148Mi\nibm-lh-lakehouse-presto-01-presto-0                50m          1568Mi\nibm-lh-postgres-edb-1                              4m           112Mi\nibm-lh-postgres-edb-2                              4m           41Mi\nibm-lh-postgres-edb-3                              4m           38Mi\nibm-nginx-8665d8c6d9-nxw66                         6m           109Mi\nibm-nginx-8665d8c6d9-qq4js                         4m           94Mi\nibm-nginx-tester-554756d6f5-hk7t4                  4m           72Mi\nlhconsole-api-76fd9f9646-shjbj                     12m          140Mi\nlhconsole-nodeclient-dbfbc4465-q2fdf               0m           31Mi\nlhconsole-ui-dd57d6c5f-bbrtj                       1m           4Mi\nspark-hb-br-recovery-6b69c86964-4srh8              2m           0Mi\nspark-hb-cloud-native-postgresql-1                 3m           61Mi\nspark-hb-cloud-native-postgresql-2                 3m           58Mi\nspark-hb-control-plane-69cf58b5cb-m6hqt            158m         1652Mi\nspark-hb-create-trust-store-58669dcd57-4f8s8       46m          56Mi\nspark-hb-deployer-agent-69c79755b8-zgwwb           2m           22Mi\nspark-hb-nginx-94cdd5474-8knzb                     0m           6Mi\nspark-hb-register-hb-dataplane-776dcbb54f-z2d47    4m           4Mi\nusermgmt-85455dbcd-7shhr                           1m           55Mi\nusermgmt-85455dbcd-snd5w                           0m           62Mi\nzen-audit-74d75cdf76-kftgw                         5m           102Mi\nzen-core-54575f65d6-9jv9d                          4m           111Mi\nzen-core-54575f65d6-wqwpc                          3m           99Mi\nzen-core-api-cb7458bbb-5f2nw                       20m          352Mi\nzen-core-api-cb7458bbb-r6qcw                       15m          346Mi\nzen-metastore-edb-1                                4m           88Mi\nzen-metastore-edb-2                                6m           81Mi\nzen-minio-0                                        32m          363Mi\nzen-minio-1                                        35m          362Mi\nzen-minio-2                                        27m          365Mi\nzen-watchdog-6b8c9c9987-jkbcr                      1m           102Mi\nzen-watcher-8b88f4858-sx4l9                        12m          273Mi\n</code></pre></p>"},{"location":"Execute%20the%20Installation%20of%20watsonx.data/#navigation_1","title":"Navigation","text":"<p>Previous Chapter or return to the Introduction.</p>"},{"location":"Optional/","title":"Troubleshoot","text":""},{"location":"Optional/#1-rebooting-the-openshift-master-node-to-fix-eof-oc-login-problem","title":"1. Rebooting the OpenShift Master Node to Fix EOF <code>oc login</code> Problem","text":"<p>If you encounter an EOF (End of File) error while using the <code>oc login</code> command in your OpenShift environment, rebooting the master node can often resolve the issue. Follow these steps:</p>"},{"location":"Optional/#12-log-in-to-bastion-node-with-user-root-not-admin","title":"1.2 Log In to Bastion Node with user root (not admin)","text":"<pre><code>ssh -i /tmp/ocp/cluster/id_rsa core@192.168.252.11\n</code></pre>"},{"location":"Optional/#13-elevate-to-root","title":"1.3 Elevate to Root","text":"<pre><code>sudo su -\n</code></pre>"},{"location":"Optional/#14-reboot-the-master-node","title":"1.4 Reboot the Master Node","text":"<pre><code>reboot\n</code></pre>"},{"location":"Optional/#2-ibm-lakehouse-manage-login-to-ocp-login-fails","title":"2. ibm-lakehouse-manage login-to-ocp login fails","text":"<p>The oc login command is working but the installer ibm-lakehouse-manage login-to-ocp fails with token expired. Please create a new token session as following:</p>"},{"location":"Optional/#21-validate-the-login","title":"2.1 Validate the login","text":"<pre><code>oc login -u kubeadmin -p $SNO_CLUSTER_ADMIN_PWD $SNO_API_URL --insecure-skip-tls-verify\n</code></pre>"},{"location":"Optional/#22-get-the-current-token","title":"2.2 Get the current Token","text":"<pre><code>export OCP_TOKEN=\"$(oc whoami -t)\"\n</code></pre>"},{"location":"Optional/#23-check-the-new-login","title":"2.3 Check the new login","text":"<p><pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage login-to-ocp \\\n--token=${OCP_TOKEN} \\\n--server=${OCP_URL}\n</code></pre> If the login with the token is not working, it is possible to execute the command with an user and password instead.</p> <pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage login-to-ocp \\\n--user=${OCP_USERNAME} \\\n--password=${OCP_PASSWORD} \\\n--server=${OCP_URL}\n</code></pre>"},{"location":"Optional/Demo_Connection_Details/","title":"Test data","text":""},{"location":"Optional/Demo_Connection_Details/#1-external-connections","title":"1. external connections","text":""},{"location":"Optional/Demo_Connection_Details/#12-aws-s3","title":"1.2 AWS S3","text":"<pre><code>- Name: Amazon S3 Connection\n- Description: Amazon S3 bucket containing archived/historic customer data.\n- Bucket: cpd-outcomes-s3\n- Endpoint URL: https://s3.us-east-2.amazonaws.com\n- Region: us-east-2\n- Authentication method: Basic credentials\n- Access key: AKIA2EJTDLF7QCA4EK4P\n- Secret key: 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 (this field will only be visible once the access key has been entered)\n</code></pre>"},{"location":"Optional/Demo_Connection_Details/#13-postgres","title":"1.3 Postgres","text":"<pre><code>- Name: Third Party Data\n- Description: Database that contains third party data needed by the business for analytics and AI.\n- Additional properties: CryptoProtocolVersion=TLSv1.2\n- Database: 3RDPARTY\n- Hostname or IP address: 85331fa6-6b56-4355-935e-290f3ac8aa8c.8117147f814b4b2ea643610826cd2046.databases.appdomain.cloud\n- Port: 31128\n- Username: cpdemo\n- Password: C!oudP@k4DataDem0s\n- Port is SSL-enabled: &lt;Checked&gt;\n- SSL certificate: &lt;Empty&gt; (default after checking that the port is SSL-enabled)\n</code></pre>"},{"location":"Optional/Demo_Connection_Details/#14-db2-warehouse","title":"1.4 Db2 Warehouse","text":"<pre><code>- Name: Data Warehouse\n- Description: Database that contains enterprise data needed by the business for analytics and AI.\n- Database: BLUDB\n- Hostname or IP address: db2w-ovqfeqq.us-south.db2w.cloud.ibm.com\n- Port: 50001\n- Authentication method: Username and password\n- Username: cpdemo\n- Password: C!oudP@k4DataDem0s\n- Port is SSL-enabled: &lt;Checked&gt; (default)\n- SSL certificate: &lt;Empty&gt; (default)\n</code></pre>"},{"location":"Optional/Demo_Connection_Details/#2-tables-details","title":"2. Tables details","text":"<ul> <li>CUSTOMER: Main customer details, including name, address, and credit card information. This table is hosted in a Db2 Warehouse relational database.</li> <li>CUSTOMER_LOYALTY: Customer loyalty and sales information from 2016 onward. This table is hosted in a PostgreSQL relational database.</li> <li> <p>CUSTOMER_LOYALTY_HISTORY: Archived customer loyalty information from 2010-2015. This data is stored in a CSV file on Amazon S3 object storage.</p> <ul> <li>QUANTITY_SOLD: Change type to INTEGER</li> <li>SHIPPING_DAYS: Change type to INTEGER</li> <li>SATISFACTION_RATING: Change type to INTEGER</li> </ul> </li> <li> <p>CUSTOMER &amp; CUSTOMER_LOYALTY = CUSTOMER_SUMMARY_V</p> <ul> <li>LOYALTY_NBR column of the CUSTOMER table and drag it to the corresponding LOYALTY_NBR row for the CUSTOMER_LOYALTY table. </li> </ul> </li> </ul>"},{"location":"Optional/Demo_Connection_Details/#3-join-tables-in-sql","title":"3. Join Tables in SQL","text":"<pre><code>SELECT \"LOYALTY_NBR\", \"ORDER_YEAR\", \"QUARTER\", \"MONTHS_AS_MEMBER\", \"LOYALTY_STATUS\", \"PRODUCT_LINE\", \"COUPON_RESPONSE\", \"COUPON_COUNT\", \"QUANTITY_SOLD\", \"UNIT_SALE_PRICE\", \"UNIT_COST\", \"REVENUE\", \"PLANNED_REVENUE\", \"SHIPPING_DAYS\", \"CUSTOMER_LIFETIME_VALUE\", \"LOYALTY_COUNT\", \"BACKORDER_STATUS\", \"SATISFACTION_RATING\", \"SATISFACTION_REASON\", \"CUSTOMER_ID\", \"FIRST_NAME\", \"LAST_NAME\", \"CUSTOMER_NAME\", \"COUNTRY\", \"STATE_NAME\", \"STATE_CODE\", \"CITY\", \"LATITUDE\", \"LONGITUDE\", \"POSTAL_CODE\", \"GENDER\", \"EDUCATION\", \"LOCATION_CODE\", \"INCOME\", \"MARITAL_STATUS\", \"CREDIT_CARD_TYPE\", \"CREDIT_CARD_NUMBER\", \"CREDIT_CARD_CVV\", \"CREDIT_CARD_EXPIRY\"\nFROM \"ALEXANDER\" . \"IBMAS-CUSTOMER_VIEW\";\n</code></pre> <pre><code>SELECT lh.\"LOYALTY_NBR\", lh.\"ORDER_YEAR\", lh.\"QUARTER\", lh.\"MONTHS_AS_MEMBER\", lh.\"LOYALTY_STATUS\", lh.\"PRODUCT_LINE\", lh.\"COUPON_RESPONSE\", lh.\"COUPON_COUNT\", lh.\"QUANTITY_SOLD\", lh.\"UNIT_SALE_PRICE\", lh.\"UNIT_COST\", lh.\"REVENUE\", lh.\"PLANNED_REVENUE\", lh.\"SHIPPING_DAYS\", lh.\"CUSTOMER_LIFETIME_VALUE\", lh.\"LOYALTY_COUNT\", lh.\"BACKORDER_STATUS\", lh.\"SATISFACTION_RATING\", lh.\"SATISFACTION_REASON\", v.\"CUSTOMER_ID\", v.\"FIRST_NAME\", v.\"LAST_NAME\", v.\"CUSTOMER_NAME\", v.\"COUNTRY\", v.\"STATE_NAME\", v.\"STATE_CODE\", v.\"CITY\", v.\"LATITUDE\", v.\"LONGITUDE\", v.\"POSTAL_CODE\", v.\"GENDER\", v.\"EDUCATION\", v.\"LOCATION_CODE\", v.\"INCOME\", v.\"MARITAL_STATUS\", v.\"CREDIT_CARD_TYPE\", v.\"CREDIT_CARD_NUMBER\", v.\"CREDIT_CARD_CVV\", v.\"CREDIT_CARD_EXPIRY\"\nFROM \"ALEXANDER\".\"IBMAS-CUSTOMER_LOYALTY_HISTORY_csv\" lh\nJOIN \"ALEXANDER\".\"IBMAS-CUSTOMER_VIEW\" v ON lh.\"LOYALTY_NBR\" = v.\"LOYALTY_NBR\";\n</code></pre>"},{"location":"Optional/get_minio_details/","title":"Optional","text":""},{"location":"Optional/get_minio_details/#1-minio-still-in-progress","title":"1. Minio (still in progress)","text":""},{"location":"Optional/get_minio_details/#12-get-internal-ibm-lakehouse-minio-information","title":"1.2 get internal IBM Lakehouse Minio information","text":"<pre><code>echo \"Access Key:\" &amp;&amp; echo \"$(oc get secret ibm-zen-objectstore-secret -n $PROJECT_CPD_INSTANCE -o go-template='{{.data.accesskey | base64decode}}')\" \\\n&amp;&amp; echo \"Secret Key:\" &amp;&amp; echo \"$(oc get secret ibm-zen-objectstore-secret -n $PROJECT_CPD_INSTANCE -o go-template='{{.data.secretkey | base64decode}}')\"\nAccess Key:\n8927889015015009\nSecret Key:\nwuuIeSjrQyHQbNTX\n</code></pre>"},{"location":"Prepare%20the%20Installation/","title":"Step 2 - Prepare the installation of watsonx.data and the bastion node","text":""},{"location":"Prepare%20the%20Installation/#navigation","title":"Navigation","text":"<p>Next Chapter: Execute the Installation of watsonx.data  or return to the Introduction.</p> <p> </p> <p>In this chapter, we'll guide you through the Bastion Node for your Red Hat Single Node Cluster (SNO). A Red Hat Single Node Cluster (SNO) is a standalone instance of a Red Hat OpenShift cluster designed to run on a single node, providing a simplified environment for testing and development purposes. It encapsulates the core capabilities of OpenShift, enabling you to simulate a multi-node cluster experience on a single machine.</p> <p>Now, let's delve into the concept of a bastion node. A bastion node, also known as a jump host or a gateway, serves as a secure entry point into a network or infrastructure. In the context of your SNO installation, the bastion node acts as an intermediary server that facilitates secure communication and management between your local machine and the Red Hat Single Node Cluster. It provides a controlled access point for performing administrative tasks, managing configurations, and executing commands within the cluster.</p> <p>Throughout this chapter, we'll outline the steps required to set up and configure the bastion node within your Red Hat Single Node Cluster, enabling you to establish a secure and manageable environment for your development and testing activities.</p>"},{"location":"Prepare%20the%20Installation/#21-prepare-the-bastion-node-for-the-installer","title":"2.1 Prepare the Bastion node for the installer","text":""},{"location":"Prepare%20the%20Installation/#22-accessing-red-hat-single-node-cluster-sno-details","title":"2.2 Accessing Red Hat Single Node Cluster (SNO) Details","text":"<p>After successfully reserving a Red Hat Single Node Cluster (SNO) through TechZone, you can proceed to access important details related to your cluster setup. To retrieve these details, follow these steps:</p> <ol> <li>Go to: Visit the TechZone platform at https://techzone.ibm.com.</li> <li>Log In: Log in to your TechZone account using your credentials.</li> <li>Access Reservations: Navigate to the \"My Reservations\" section. You can usually find this in your account dashboard or a similar location.</li> <li>Select Current Image: Locate and select your current Single Node OpenShift (VMware on IBM Cloud) image reservation from the list. This should correspond to the Red Hat Single Node Cluster you reserved.</li> <li>Retrieve Cluster Details: Once you've selected your reservation, you should see a detailed view of your reserved SNO image. Look for information such as:</li> <li>User and Password: Credentials to access your cluster. (user: kubeadmin)</li> <li>OpenShift Public URL: The URL to access the OpenShift web console.</li> <li>Bastion Node Terminal (SSH) Details: Information on how to SSH into the bastion node, which serves as a secure entry point into your cluster. </li> <li>Accessing the Bastion Node: Use the provided SSH details to connect to the bastion node. From the bastion node, you can perform administrative tasks, manage configurations, and execute commands within your SNO cluster. (user: admin and sudo su -)</li> </ol> <p>By following these steps and retrieving the cluster details from the TechZone reservation page, you'll have the necessary information to effectively manage and work with your Red Hat Single Node Cluster environment.</p>"},{"location":"Prepare%20the%20Installation/#221-logon-to-the-openshift-console-example-from-reservation-details","title":"2.2.1 Logon to the openshift console (example from reservation details)","text":"<p>Example: <pre><code>https://console-openshift-console.apps.64da1ffc1bedbf00175f38c9.cloud.techzone.ibm.com\nkubeadmin\nzR6vy-FvZXh-IzfCn-SIG4x\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#222-logon-to-the-bastion-node-example-from-reservation-details","title":"2.2.2 Logon to the bastion node (example from reservation details)","text":"<p>Prepare the bastion node via SSH (details in the techzone reservation)  Example: <pre><code>ssh admin@api.64da1ffc1bedbf00175f38c9.cloud.techzone.ibm.com -p 40222\nPassword: yDVe43J8\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#223-switch-from-user-admin-to-root-via-sudo","title":"2.2.3 switch from user admin to root via sudo","text":"<pre><code>sudo su -\n</code></pre>"},{"location":"Prepare%20the%20Installation/#3-install-required-redhat-base-software","title":"3. Install required Redhat base software","text":"<p>Install NFS and the screen software to set up NFS storage for watsonx.data, facilitating seamless access to storage resources while ensuring security through firewall services. This installation process can be conveniently executed using the screen utility, which operates as a background terminal service. This eliminates the need to append commands with &amp; or utilize nohup, streamlining the configuration process. <pre><code>yum -y install nfs-utils screen\nsystemctl enable --now nfs-server rpcbind\nsystemctl start nfs-server\nmkdir /export\ntouch /etc/exports\necho \"/export *(rw,sync,no_root_squash,no_all_squash)\" &gt;&gt; /etc/exports\nfirewall-cmd --add-service=nfs --permanent\nfirewall-cmd --add-service={nfs3,mountd,rpc-bind} --permanent\nfirewall-cmd --reload\nsystemctl restart nfs-server\nsystemctl status nfs-server\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#4-install-the-cloudpak-installer-cli-package","title":"4. Install the Cloudpak Installer CLI package","text":"<p>Stay up to date with the latest version of the IBM Installer by visiting the official GitHub repository at https://github.com/IBM/cpd-cli/releases. This ensures that you have access to the most current features and enhancements for a seamless installation experience. <pre><code>wget https://github.com/IBM/cpd-cli/releases/download/v13.0.1/cpd-cli-linux-EE-13.0.1.tgz\ntar -xzvf cpd-cli-linux-EE-13.0.1.tgz\nmv cpd-cli-linux-EE-13.0.1-26/* .\nrm -rf cpd-cli-linux-EE-13.0.1-26\nrm -f cpd-cli-linux-EE-13.0.1.tgz\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#41-retrieve-the-api-connection-string-for-accessing-openshift-by-obtaining-the-red-hat-https-api-url","title":"4.1  Retrieve the API connection string for accessing OpenShift by obtaining the Red Hat HTTPS API URL.","text":""},{"location":"Prepare%20the%20Installation/#412-how-to-get-the-openshift-api-url","title":"4.1.2 How to Get the OpenShift API URL","text":"<p>To obtain the API URL of an OpenShift cluster, follow these steps:</p> <ol> <li>Access OpenShift Web Console: Open a web browser (Chrome is recommended) and navigate to the OpenShift Web Console. This typically involves entering the URL provided by your cluster administrator.</li> <li>Log In: Log in to the OpenShift Web Console using your credentials.</li> <li>Navigate to Cluster Details: Once logged in, navigate to the \"Help\" or \"User\" section of the OpenShift Web Console. Look for an option that provides cluster details or settings.</li> <li>Find API URL: In the cluster details or settings section, you should find information about the API server URL. It may be labeled as \"API URL,\" \"Cluster URL,\" or something similar. This URL is the endpoint you'll use to interact with the OpenShift cluster programmatically.</li> <li>Copy the API URL: Copy the API URL to your clipboard. It will typically start with <code>https://</code> and include the cluster's domain name or IP address.</li> <li>Use the API URL: You can now use the copied API URL to interact with the OpenShift cluster through various tools, scripts, or applications that communicate with the OpenShift API. By following these steps, you'll be able to retrieve the API URL of your OpenShift cluster, allowing you to efficiently manage and interact with the cluster's resources programmatically.</li> </ol> <p> logon to your cluster Desktop/Console URL and check for the cluster informations</p>"},{"location":"Prepare%20the%20Installation/#413-get-the-ibm-entitlement-key","title":"4.1.3 Get the IBM Entitlement Key","text":"<p> Getting an API key to download the installation images. This API key will provide you access to the IBM Container Library where you can obtain the required installation images. Click on the badge above or visit the IBM Container Library to obtain your API key.</p>"},{"location":"Prepare%20the%20Installation/#414-configure-the-data-as-specified-in-the-techzone-reservation-details","title":"4.1.4 Configure the data as specified in the TechZone reservation details:","text":"<p><pre><code>export SNO_API_URL=&lt;API URL&gt;:&lt;PORT&gt;\nexport SNO_CLUSTER_ADMIN_PWD=&lt;kubeadmin Password&gt;\nexport SNO_IBM_ENTITLEMENT_KEY=&lt;IBM Entitlement Key from section 4.1.3)\n</code></pre> Example: <pre><code>export SNO_API_URL=https://api.64da1ffc1bedbf00175f38c9.cloud.techzone.ibm.com:6443\nexport SNO_CLUSTER_ADMIN_PWD=zR6vy-FvZXh-IzfCn-SIG4x\nexport SNO_IBM_ENTITLEMENT_KEY=eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE2MDY0NzEzNTksImp0aSI6IjkzNGY1ZjMxNTBjZjRiMjBhNTI0ZTA2MmJkZjNlNmRhIn0._4cHQE3w3iDhpKZocW0bL376zNG3ebzqYcJINNUUS7w\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#5-update-the-hosts-for-to-the-api-connection-string","title":"5 Update the hosts for to the API connection string","text":"<pre><code>export SNO_API_HOST=$(echo $SNO_API_URL | sed 's/https:\\/\\///g' | sed 's/:6443//g')\necho \"192.168.252.1 $SNO_API_HOST\" &gt;&gt; /etc/hosts\n</code></pre>"},{"location":"Prepare%20the%20Installation/#51-confirm-the-successful-login-to-the-openshift-cluster","title":"5.1 Confirm the successful login to the OpenShift cluster.","text":"<pre><code>oc login -u kubeadmin -p $SNO_CLUSTER_ADMIN_PWD $SNO_API_URL --insecure-skip-tls-verify\n</code></pre>"},{"location":"Prepare%20the%20Installation/#6-initiate-the-environment-setup-using-the-unix-environment-to-configure-the-installation-settings","title":"6. Initiate the Environment Setup using the Unix environment to configure the installation settings.","text":"<p>The current version of IBM Lakehouse (watsonx.data) is 1.0.1 this can be change from LH_IMAGE_TAG=latest to LH_IMAGE_TAG=1.0.1.  All operators are installed in watsonxdata1-operator namespace and the instance itself in watsonxdata1-instance. Nothing else needs to be replaced for this demo. <pre><code>mkdir /root/ibm-lh-manage\ncd /root/ibm-lh-manage/\n</code></pre> <pre><code>tee ibm-lh-manage.env &lt;&lt;EOF \n# ------------------------------------------------------------------------------\n# Watsonx.data  version\n# ------------------------------------------------------------------------------\nexport LH_IMAGE_TAG=latest\n# ------------------------------------------------------------------------------\n# Watsonx.data settings\n# ------------------------------------------------------------------------------\nexport DOCKER_EXE=podman\nexport UTILS_IMG=icr.io/cpopen/watsonx-data/ibm-lakehouse-manage-utils:\\$LH_IMAGE_TAG\nexport IBM_LH_TOOLBOX=icr.io/cpopen/watsonx-data/ibm-lakehouse-toolbox:\\$LH_IMAGE_TAG\nexport WORK_DIR=/root/ibm-lh-manage/.ibm-lh-manage-utils\nexport IMAGE_ARCH=x86-64\nexport PROD_REGISTRY=cp.icr.io\nexport PROD_USER=cp\n# ------------------------------------------------------------------------------\n# Projects\n# ------------------------------------------------------------------------------\nexport PROJECT_CPD_OPS=watsonxdata1-operator\nexport PROJECT_CPD_INSTANCE=watsonxdata1-instance\n# ------------------------------------------------------------------------------\n# IBM Entitled Registry\n# ------------------------------------------------------------------------------\nexport IBM_ENTITLEMENT_KEY=$SNO_IBM_ENTITLEMENT_KEY\n# ------------------------------------------------------------------------------\n# Cluster\n# ------------------------------------------------------------------------------\nexport OCP_USERNAME=\"kubeadmin\"\nexport OCP_PASSWORD=$SNO_CLUSTER_ADMIN_PWD\nexport OCP_TOKEN=\"$(oc whoami -t)\"\nexport OCP_URL=$SNO_API_URL\nexport OPENSHIFT_TYPE=self-managed\n# ------------------------------------------------------------------------------\n# NFS Storage\n# ------------------------------------------------------------------------------\nexport NFS_SERVER_LOCATION=192.168.252.2\nexport NFS_PATH=/export\nexport PROJECT_NFS_PROVISIONER=nfs-provisioner\nexport NFS_STORAGE_CLASS=nfs-storage-provisioner\nexport NFS_IMAGE=k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\nexport STG_CLASS_BLOCK=nfs-storage-provisioner\nexport STG_CLASS_FILE=nfs-storage-provisioner\nEOF\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#61-activate-the-settings-using-the-bash-shell-and-source-the-environment","title":"6.1 Activate the settings using the bash shell and source the environment.","text":"<pre><code>source /root/ibm-lh-manage/ibm-lh-manage.env\necho \"source /root/ibm-lh-manage/ibm-lh-manage.env\" &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"Prepare%20the%20Installation/#7-pull-the-watsonxdata-ibm-lakehouse-toolbox-image","title":"7.  Pull the watsonx.data ibm-lakehouse-toolbox image.","text":"<pre><code>$DOCKER_EXE pull $IBM_LH_TOOLBOX\nid=$($DOCKER_EXE create $IBM_LH_TOOLBOX)\n$DOCKER_EXE cp $id:/opt - &gt; /tmp/pkg.tar\n$DOCKER_EXE rm $id\nunset id\n</code></pre>"},{"location":"Prepare%20the%20Installation/#71-extract-the-watsonxdata-stand-alone-pkgtar-file-in-to-the-tmp-directory-verify-that-the-checksum-is-correct-by-comparing-the-checksum","title":"7.1 Extract the watsonx.data stand-alone pkg.tar file in to the /tmp directory. Verify that the checksum is correct by comparing the checksum","text":"<pre><code>tar -xf /tmp/pkg.tar -C /tmp\ncat /tmp/opt/bom.txt\ncksum /tmp/opt/*/*\ncp /tmp/opt/standalone/ibm-lakehouse-manage /root/ibm-lh-manage\ncp /tmp/opt/standalone/README.txt /root/ibm-lh-manage\n</code></pre>"},{"location":"Prepare%20the%20Installation/#8-run-the-following-command-to-initialize-the-ibm-lh-manage-utils-container","title":"8. Run the following command to initialize the ibm-lh-manage-utils container.","text":"<pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage initialize\n</code></pre>"},{"location":"Prepare%20the%20Installation/#81-validate-pod-optional","title":"8.1 validate pod (optional):","text":"<pre><code>podman exec -ti ibm-lakehouse-manage-utils bash  \n</code></pre>"},{"location":"Prepare%20the%20Installation/#82-validate-the-login","title":"8.2 validate the login:","text":"<pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage login-to-ocp \\\n--token=${OCP_TOKEN} \\\n--server=${OCP_URL}\n</code></pre>"},{"location":"Prepare%20the%20Installation/#83-install-nfs-provisioner-and-create-a-storage-class-for-watsonxdata","title":"8.3 Install NFS Provisioner and create a storage class for Watsonx.data","text":"<pre><code>./ibm-lakehouse-manage setup-nfs-provisioner \\\n--nfs_server=${NFS_SERVER_LOCATION} \\\n--nfs_path=${NFS_PATH} \\\n--nfs_provisioner_ns=${PROJECT_NFS_PROVISIONER} \\\n--nfs_storageclass_name=${NFS_STORAGE_CLASS} \\\n--nfs_provisioner_image=${NFS_IMAGE}\n</code></pre>"},{"location":"Prepare%20the%20Installation/#84-pause-for-the-patches","title":"8.4 pause for the patches","text":"<pre><code>oc patch --type=merge --patch='{\"spec\":{\"paused\":true}}' machineconfigpool/master\noc patch --type=merge --patch='{\"spec\":{\"paused\":true}}' machineconfigpool/worker\n</code></pre>"},{"location":"Prepare%20the%20Installation/#8411-add-the-pull-secret-to-the-artifactory-that-contains-watsonxdata-images","title":"8.4.1.1 Add the pull secret to the artifactory that contains watsonx.data images.","text":"<pre><code>/root/ibm-lh-manage/ibm-lakehouse-manage add-icr-cred-to-global-pull-secret --entitled_registry_key=${IBM_ENTITLEMENT_KEY}\n</code></pre>"},{"location":"Prepare%20the%20Installation/#8412-important-change-the-openshift-pod-limit-from-250-to-320","title":"8.4.1.2 Important: Change the Openshift POD limit from 250 to 320","text":"<p>to avoid temporary issues, such as oc login is not possible anymore, you need to increase the pod limit for this single node cluster. <pre><code>oc label --overwrite machineconfigpool master custom-kubelet=large-pods\noc apply -f - &lt;&lt;EOF\napiVersion: machineconfiguration.openshift.io/v1\nkind: KubeletConfig\nmetadata:\nname: \"set-max-pods\"\nspec:\nmachineConfigPoolSelector:\nmatchLabels:\ncustom-kubelet: large-pods\nkubeletConfig:\nmaxPods: 320\nEOF\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#842-check-the-update-rollout","title":"8.4.2 Check the Update Rollout","text":"<p>When the pull secret is created, Red Hat OpenShift propagates it to every node that might take some time to complete. Therefore, wait until the UPDATED column displays True for all the worker nodes in the system config pool before you proceed to the next step. <pre><code>watch oc get mcp\n</code></pre> Output: <pre><code>NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nmaster   rendered-master-2df1fc6555c56bbafbc513f89eac366c   False     False  False      1              0                   0                     0                      112m\nworker   rendered-worker-5920c72cbaf105641bbd46b714c4c3ef   True      False  False      0              0                   0                     0                      112m\n</code></pre></p>"},{"location":"Prepare%20the%20Installation/#843-un-pause-for-the-patches","title":"8.4.3 un pause for the patches","text":"<pre><code>oc patch --type=merge --patch='{\"spec\":{\"paused\":false}}' machineconfigpool/master\noc patch --type=merge --patch='{\"spec\":{\"paused\":false}}' machineconfigpool/worker\n</code></pre>"},{"location":"Prepare%20the%20Installation/#navigation_1","title":"Navigation","text":"<p>Next Chapter: Execute the Installation of watsonx.data  or return to the Introduction.</p>"},{"location":"Requirements/","title":"Step 1 - Requirements for Watsonx.data Installation","text":""},{"location":"Requirements/#navigation","title":"Navigation","text":"<p>Next Chapter: Prepare the Installation </p> <p>Before proceeding with the installation of watsonx.data, it's important to ensure that your environment meets the necessary prerequisites. This section outlines the key requirements that need to be addressed to successfully deploy watsonx.data on your Red Hat cluster. These requirements lay the foundation for a smooth and efficient installation process, allowing you to leverage the capabilities of watsonx.data seamlessly.</p> <p>In this section, we will cover essential aspects such as provisioning a Red Hat cluster on the TechZone platform, which serves as the underlying infrastructure for your watsonx.data deployment. Properly fulfilling these requirements will help create a robust foundation for your processing tasks using watsonx.data.</p> <p>Let's begin by exploring the steps involved in meeting these prerequisites and preparing your environment for the installation of watsonx.data.</p>"},{"location":"Requirements/#2-summary-for-prerequisites","title":"2. Summary for prerequisites","text":"<p>Before you begin the installation process, make sure you have the following prerequisites in place:</p> <ol> <li> <p>IBMid Credentials: Have your IBMid credentials ready to log in and access the necessary content on TechZone.</p> </li> <li> <p>TechZone Access: Ensure access to techzone.ibm.com, where you'll find essential information and resources related to the installation.</p> </li> <li> <p>IBM Entitlement Key: Keep your IBM Entitlement Key handy, as it will be required during the installation for accessing IBM resources.</p> </li> <li> <p>Terminal or Putty: Have a terminal session or Putty installed to execute commands and manage the installation.</p> </li> <li> <p>Chrome Browser: Use Chrome as your preferred browser (avoid Safari on Mac) for optimal compatibility with the installation process.</p> </li> </ol> <p>With these prerequisites fulfilled, you'll be well-equipped to navigate through the installation process smoothly and efficiently.</p>"},{"location":"Requirements/#3-useful-links","title":"3. Useful Links","text":""},{"location":"Requirements/#31-techzone-business-partner-access-information","title":"3.1 Techzone Business Partner Access Information","text":"<p>Learn about accessing Techzone as a Business Partner. Business Partners (BPs) Authenticated on IBM Partner Plus (PP) Have Access to IBM Technology Zone Using Their IBM Ids.</p>"},{"location":"Requirements/#32-main-installation-documentation-watson-xdata","title":"3.2 Main Installation Documentation: Watson x.data","text":"<p>IBM\u00ae watsonx.data is a new open architecture lakehouse that combines the elements of the data warehouse and data lakes. The best-in-class features and optimizations available on the watsonx.data make it an optimal choice for next generation data analytics and automation.</p> <p>watsonx.data is a unique solution that allows co-existence of open source technologies and proprietary products. It offers a single platform where you can store the data or attach data sources for managing and analyzing your enterprise data.</p> <p>Use watsonx.data to store any type of data (structured, semi-structured, and unstructured) and make that data accessible directly for Artificial Intelligence (AI) and Business Intelligence (BI). You can also attach your data sources to watsonx.data, which helps to reduce data duplication and cost of storing data in multiple places. It uses open data formats with APIs and machine learning libraries, making it easier for data scientists and data engineers to use the data. watsonx.data architecture enforces schema and data integrity, making it easier to implement robust data security and governance mechanisms.</p>"},{"location":"Requirements/#33-getting-an-ibm-entitlement-key-to-download-installation-images","title":"3.3 Getting an IBM Entitlement Key to Download Installation Images","text":"<p>Obtain an API key to download installation images. Access your container software. With your entitlement key, you can access all of your container software in the IBM Entitled Registry. A complete list of container software that you own can be found in the Container Software Library.</p> <p>Use any active entitlement key to log in to the image registry and retrieve any container software that you own. \u2014 You can have a maximum of (5) entitlement keys. \u2014 Once a key is deleted, it is no longer valid.</p>"},{"location":"Requirements/#34-original-and-main-information-about-sno-for-cloudpak-for-data","title":"3.4 Original and Main Information about SNO for Cloudpak for Data","text":"<p>Access the original documentation about a Single Node Openshift Cluster for a Cloudpak for Data Installation.</p>"},{"location":"Requirements/#4-important","title":"4. Important:","text":"<ul> <li>request a 32Core x 128GB Memory</li> <li>do not select FIPS (must be disabled)</li> </ul>"},{"location":"Requirements/#41-to-start-with-the-installation-you-need-to-request-a-single-node-cluster-instance-at-ibm-techzone","title":"4.1 To start with the Installation you need to request a Single Node Cluster Instance at IBM Techzone:","text":"<ul> <li>Go to Single Node OpenShift (VMware on IBM Cloud)</li> <li>Click Reserve now</li> <li>Select Purpose -&gt; Practice / Self-Eduction</li> <li>Click the checkbox to confirm that no customer data is being used</li> <li>Leave the opportunity number empty</li> <li>Enter a description for the Purpose</li> <li>Choose the geography DAL12 (other geographies are WDC04, LON02, LON06, FRA04 and TOK02)</li> <li>Leave the defaults for \"End date time\", \"OCP/Kubernetes Cluster Network\" and \"Disable FIPS Security\"</li> <li>Choose the master single node flavor. Choose at least the 32x256 flavor</li> <li>Choose the OpenShift version 4.12</li> <li>Leave the defaults for \"OCP/Kubernetes Service Network\"</li> <li>Eventually add notes</li> <li>Click Submit</li> </ul>"},{"location":"Requirements/#navigation_1","title":"Navigation","text":"<p>Next Chapter: Prepare the Installation </p>"}]}